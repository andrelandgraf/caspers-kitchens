# databricks.yml
bundle:
  name: caspers-kitchens

variables:
  catalog:
    description: "UC catalog for this bundle/target"
    default: caspersdev

workspace:
  # Default root path under your user home; still configurable per target
  root_path: /Workspace/Users/${workspace.current_user.userName}/caspers-kitchens-demo
  file_path: ${workspace.root_path}     # where files get synced

# Ship the repo files on deploy (adjust excludes as you like)
sync:
  include:
    - ./*
  exclude:
    - .git/**
    - .DS_Store
    - .claude/**
    - .databricks/**
    - apps/supportconsolek/**/.databricks/**
    - apps/supportconsolek/**/node_modules/**
    - apps/supportconsolek/**/dist/**
    - apps/supportconsolek/**/client/dist/**
    - ./data/universe/**

scripts:
  cleanup:
    content: |
      set -eo pipefail

      # Detect the target passed to `databricks bundle run cleanup -t <target>`
      TARGET_FLAG=""
      if env | grep -q '^DATABRICKS_BUNDLE_TARGET='; then
        TARGET_FLAG="-t $DATABRICKS_BUNDLE_TARGET"
      fi

      # Get fully-resolved bundle config (includes target overrides)
      RESOLVED_JSON="$(databricks bundle validate $TARGET_FLAG --output json)"

      # Pull values we need (requires jq)
      FILE_PATH="$(printf '%s' "$RESOLVED_JSON" | jq -r '.workspace.file_path // (.workspace.root_path + "/src")')"
      CATALOG="$(printf '%s' "$RESOLVED_JSON" | jq -r '.variables.catalog.value // .variables.catalog.default')"

      mkdir -p .bundle
      cat > .bundle/submit.json <<JSON
      {
        "run_name": "caspers: cleanup (ephemeral)",
        "performance_target": "PERFORMANCE_OPTIMIZED",
        "tasks": [
          {
            "task_key": "destroy",
            "notebook_task": {
              "notebook_path": "$FILE_PATH/destroy",
              "source": "WORKSPACE",
              "base_parameters": { "CATALOG": "$CATALOG" }
            }
          }
        ]
      }
      JSON

      databricks jobs submit --json @.bundle/submit.json


targets:
  default:
    default: true
    resources:
      jobs:
        caspers:
          name: "Casper's Initializer"
          queue:
            enabled: true
          performance_target: PERFORMANCE_OPTIMIZED
          parameters:
            - name: CATALOG
              default: ${var.catalog}
            - name: EVENTS_VOLUME
              default: events
            - name: SIMULATOR_SCHEMA
              default: simulator
            - name: START_DAY
              default: "70"
            - name: SPEED_MULTIPLIER
              default: "60.0"
            - name: SCHEDULE_MINUTES
              default: "3"
            - name: PIPELINE_SCHEDULE_MINUTES
              default: "3"
          tasks:
            - task_key: Canonical_Data
              notebook_task:
                notebook_path: ${workspace.root_path}/stages/canonical_data
            - task_key: Spark_Declarative_Pipeline
              depends_on:
                - task_key: Canonical_Data
              notebook_task:
                notebook_path: ${workspace.root_path}/stages/lakeflow

  support:
    resources:
      jobs:
        caspers:
          name: "Casper's Initializer"
          queue:
            enabled: true
          performance_target: PERFORMANCE_OPTIMIZED
          parameters:
            - name: CATALOG
              default: casperskitchens
            - name: EVENTS_VOLUME
              default: events
            - name: LOCATIONS
              default: sanfrancisco.json
            - name: LLM_MODEL
              default: databricks-meta-llama-3-3-70b-instruct
            - name: SUPPORT_AGENT_ENDPOINT_NAME
              default: caspers_support_agent
            - name: SUPPORT_RATE
              default: "0.18"
            - name: SIMULATOR_SCHEMA
              default: simulator
            - name: START_DAY
              default: "70"
            - name: SPEED_MULTIPLIER
              default: "60.0"
            - name: SCHEDULE_MINUTES
              default: "3"
            - name: PIPELINE_SCHEDULE_MINUTES
              default: "0"
          tasks:
            - task_key: Raw_Data
              notebook_task:
                notebook_path: ${workspace.root_path}/stages/raw_data
            - task_key: Spark_Declarative_Pipeline
              depends_on:
                - task_key: Raw_Data
              notebook_task:
                notebook_path: ${workspace.root_path}/stages/lakeflow
            - task_key: Support_Request_Agent
              depends_on:
                - task_key: Spark_Declarative_Pipeline
              notebook_task:
                notebook_path: ${workspace.root_path}/stages/support_request_agent
            - task_key: Support_Request_Generator_Stream
              depends_on:
                - task_key: Spark_Declarative_Pipeline
              notebook_task:
                notebook_path: ${workspace.root_path}/stages/support_request_generator_stream
            - task_key: Support_Request_Agent_Stream
              depends_on:
                - task_key: Support_Request_Agent
                - task_key: Support_Request_Generator_Stream
              notebook_task:
                notebook_path: ${workspace.root_path}/stages/support_request_agent_stream
            - task_key: Support_Lakebase
              depends_on:
                - task_key: Support_Request_Agent_Stream
              notebook_task:
                notebook_path: ${workspace.root_path}/stages/support_lakebase
        support_response_evals_hourly:
          name: "Support Response Evals Hourly"
          queue:
            enabled: true
          performance_target: PERFORMANCE_OPTIMIZED
          parameters:
            - name: CATALOG
              default: casperskitchens
            - name: JUDGE_MODEL
              default: databricks-gpt-5-mini
            - name: EVAL_LIMIT
              default: "100"
          schedule:
            quartz_cron_expression: "0 0 * * * ?"
            timezone_id: "UTC"
            pause_status: "UNPAUSED"
          tasks:
            - task_key: run_support_response_evals
              notebook_task:
                notebook_path: ${workspace.root_path}/demos/agent-compare-models/demo_materials/support-response-evals
                base_parameters:
                  CATALOG: "{{job.parameters.CATALOG}}"
                  JUDGE_MODEL: "{{job.parameters.JUDGE_MODEL}}"
                  EVAL_LIMIT: "{{job.parameters.EVAL_LIMIT}}"
