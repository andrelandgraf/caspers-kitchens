{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Response Evals (Casper's Kitchens)\n",
    "\n",
    "This notebook shows how to evaluate support agent responses with MLflow judges and compare judge outcomes against human thumbs ratings captured in the support app.\n",
    "\n",
    "## What this covers\n",
    "\n",
    "1. Build an evaluation dataset from support requests and latest agent responses\n",
    "2. Load human ratings (`thumbs_up` / `thumbs_down`) from Lakebase\n",
    "3. Run `mlflow.genai.evaluate()` with an LLM judge\n",
    "4. Compute `human_agreement_rate` between judge outcomes and app ratings\n",
    "5. Review results in Databricks UI (MLflow traces + evaluation views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow[databricks] databricks-sdk psycopg2-binary pandas\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import psycopg2\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from mlflow.genai.judges import make_judge\n",
    "\n",
    "dbutils.widgets.text(\"CATALOG\", \"caspersdev\", \"UC Catalog\")\n",
    "dbutils.widgets.text(\"JUDGE_MODEL\", \"databricks-gpt-5-mini\", \"Judge model endpoint\")\n",
    "dbutils.widgets.text(\"EVAL_LIMIT\", \"50\", \"Max examples\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "JUDGE_MODEL = dbutils.widgets.get(\"JUDGE_MODEL\")\n",
    "EVAL_LIMIT = int(dbutils.widgets.get(\"EVAL_LIMIT\") or \"50\")\n",
    "\n",
    "experiment = mlflow.set_experiment(f\"/Shared/{CATALOG}_support_response_evals\")\n",
    "print(f\"Using experiment: {experiment.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest support response per request and join request text.\n",
    "support_df = spark.sql(f\"\"\"\n",
    "WITH latest_reports AS (\n",
    "  SELECT support_request_id, user_id, order_id, ts, agent_response,\n",
    "         ROW_NUMBER() OVER (PARTITION BY support_request_id ORDER BY ts DESC) AS rn\n",
    "  FROM {CATALOG}.support.support_agent_reports_sync\n",
    ")\n",
    "SELECT\n",
    "  l.support_request_id,\n",
    "  l.user_id,\n",
    "  l.order_id,\n",
    "  l.ts,\n",
    "  l.agent_response,\n",
    "  r.request_text\n",
    "FROM latest_reports l\n",
    "LEFT JOIN {CATALOG}.support.raw_support_requests r\n",
    "  ON r.support_request_id = l.support_request_id\n",
    "WHERE l.rn = 1\n",
    "ORDER BY l.ts DESC\n",
    "LIMIT {EVAL_LIMIT}\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"Loaded {len(support_df)} support responses\")\n",
    "support_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest human rating from Lakebase support.response_ratings.\n",
    "config_row = spark.sql(f\"\"\"\n",
    "SELECT endpoint_name, endpoint_host, database_name\n",
    "FROM {CATALOG}.support.lakebase_v2_config\n",
    "ORDER BY updated_at DESC\n",
    "LIMIT 1\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "endpoint_name = config_row[\"endpoint_name\"]\n",
    "endpoint_host = config_row[\"endpoint_host\"]\n",
    "database_name = config_row[\"database_name\"]\n",
    "\n",
    "w = WorkspaceClient()\n",
    "creds = w.postgres.generate_database_credential(endpoint=endpoint_name)\n",
    "current_user = w.current_user.me().user_name\n",
    "\n",
    "ratings_df = pd.DataFrame(\n",
    "    columns=[\"support_request_id\", \"rating\", \"reason_code\", \"feedback_notes\", \"actor\", \"created_at\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=endpoint_host,\n",
    "        port=5432,\n",
    "        dbname=database_name,\n",
    "        user=current_user,\n",
    "        password=creds.token,\n",
    "        sslmode=\"require\",\n",
    "    )\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT support_request_id, rating, reason_code, feedback_notes, actor, created_at\n",
    "            FROM support.response_ratings\n",
    "            ORDER BY created_at DESC\n",
    "            \"\"\"\n",
    "        )\n",
    "        rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    ratings_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"support_request_id\", \"rating\", \"reason_code\", \"feedback_notes\", \"actor\", \"created_at\"],\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Could not load ratings yet: {e}\")\n",
    "\n",
    "if not ratings_df.empty:\n",
    "    ratings_df = (\n",
    "        ratings_df.sort_values(\"created_at\", ascending=False)\n",
    "        .drop_duplicates(subset=[\"support_request_id\"], keep=\"first\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "print(f\"Loaded {len(ratings_df)} latest human ratings\")\n",
    "ratings_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_report(raw):\n",
    "    if isinstance(raw, dict):\n",
    "        return raw\n",
    "    if isinstance(raw, str):\n",
    "        try:\n",
    "            return json.loads(raw)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "eval_data = []\n",
    "for _, row in support_df.iterrows():\n",
    "    report = parse_report(row.get(\"agent_response\"))\n",
    "    eval_data.append(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"support_request_id\": row.get(\"support_request_id\"),\n",
    "                \"order_id\": row.get(\"order_id\"),\n",
    "                \"request_text\": row.get(\"request_text\") or \"\",\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"draft_response\": report.get(\"draft_response\"),\n",
    "                \"refund_recommendation\": report.get(\"refund_recommendation\"),\n",
    "                \"credit_recommendation\": report.get(\"credit_recommendation\"),\n",
    "                \"decision_confidence\": report.get(\"decision_confidence\"),\n",
    "                \"escalation_flag\": report.get(\"escalation_flag\"),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Prepared {len(eval_data)} eval rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_compliance_judge = make_judge(\n",
    "    name=\"support_policy_compliance\",\n",
    "    instructions=\"\"\"\n",
    "Evaluate whether the support response is policy-compliant and actionable.\n",
    "\n",
    "Support case input: {{ inputs }}\n",
    "Agent output: {{ outputs }}\n",
    "\n",
    "Pass criteria:\n",
    "- Draft response is professional and directly addresses the complaint.\n",
    "- If refund/credit is recommended, amount or rationale is coherent.\n",
    "- Response does not expose internal system identifiers.\n",
    "- Escalation flag and confidence are consistent with severity.\n",
    "\n",
    "Return PASS only when all major criteria are satisfied. Otherwise return FAIL.\n",
    "\"\"\",\n",
    "    model=f\"databricks:/{JUDGE_MODEL}\",\n",
    ")\n",
    "\n",
    "evaluation_result = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    scorers=[policy_compliance_judge],\n",
    ")\n",
    "\n",
    "print(\"Evaluation completed\")\n",
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas_table(obj):\n",
    "    if obj is None:\n",
    "        return None\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj\n",
    "    if hasattr(obj, \"toPandas\"):\n",
    "        return obj.toPandas()\n",
    "    return None\n",
    "\n",
    "eval_df = None\n",
    "if hasattr(evaluation_result, \"tables\") and isinstance(evaluation_result.tables, dict):\n",
    "    for _, table in evaluation_result.tables.items():\n",
    "        candidate = to_pandas_table(table)\n",
    "        if candidate is not None and len(candidate) > 0:\n",
    "            eval_df = candidate\n",
    "            break\n",
    "\n",
    "if eval_df is None:\n",
    "    raise RuntimeError(\"Could not extract evaluation rows from EvaluationResult.tables\")\n",
    "\n",
    "judge_value_col = next((c for c in eval_df.columns if \"support_policy_compliance\" in c and c.endswith(\"value\")), None)\n",
    "inputs_col = \"inputs\" if \"inputs\" in eval_df.columns else None\n",
    "\n",
    "if judge_value_col is None or inputs_col is None:\n",
    "    print(\"Available columns:\", list(eval_df.columns))\n",
    "    raise RuntimeError(\"Expected evaluation columns were not found\")\n",
    "\n",
    "def parse_inputs(value):\n",
    "    if isinstance(value, dict):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return json.loads(value)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "eval_df[\"support_request_id\"] = eval_df[inputs_col].apply(lambda v: parse_inputs(v).get(\"support_request_id\"))\n",
    "eval_df[\"judge_label\"] = eval_df[judge_value_col].astype(str).str.upper().map({\"PASS\": \"thumbs_up\", \"FAIL\": \"thumbs_down\"})\n",
    "\n",
    "if ratings_df.empty:\n",
    "    print(\"No human ratings yet. Submit ratings in the support app, then rerun this cell.\")\n",
    "else:\n",
    "    merged = eval_df[[\"support_request_id\", \"judge_label\"]].merge(\n",
    "        ratings_df[[\"support_request_id\", \"rating\"]],\n",
    "        on=\"support_request_id\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    if len(merged) == 0:\n",
    "        print(\"No overlap yet between evaluated rows and rated rows.\")\n",
    "    else:\n",
    "        merged[\"agrees\"] = merged[\"judge_label\"] == merged[\"rating\"]\n",
    "        human_agreement_rate = float(merged[\"agrees\"].mean())\n",
    "        print(f\"human_agreement_rate: {human_agreement_rate:.2%} ({merged['agrees'].sum()}/{len(merged)})\")\n",
    "        display(merged.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to review in Databricks UI\n",
    "\n",
    "1. **Support app UI**\n",
    "   - Open a support request in the details drawer.\n",
    "   - Use **Rate Agent Response** to save thumbs up/down (+ optional reason and notes).\n",
    "\n",
    "2. **Lakebase ratings table**\n",
    "   - Query `support.response_ratings` to audit latest operator feedback per request.\n",
    "\n",
    "3. **MLflow experiment view** (`/Shared/<CATALOG>_support_response_evals`)\n",
    "   - **Runs**: see each `mlflow.genai.evaluate` execution.\n",
    "   - **Evaluation** tab: see scorer pass/fail values and judge rationales.\n",
    "   - **Traces** tab / trace detail: inspect per-example records and add human assessments if you want judge alignment.\n",
    "\n",
    "4. **Judge alignment (optional)**\n",
    "   - After adding human assessments in trace details, align the template judge with `judge.align(...)` in a follow-up run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
