{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "14a10bae-42e6-4e06-81fa-c6d2336d157a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "\n",
        "### Raw Data\n",
        "\n",
        "This notebook assumes raw_data has already run and creates a medallion architecture declarative pipeline to normalize the event stream and create summary tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "2953476c-3a7a-406c-b3f7-c1b45f1f7ca0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "%pip install --upgrade databricks-sdk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e8704c18-8794-4ac2-8d7e-1c69bb3b4371",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
        "EVENTS_VOLUME = dbutils.widgets.get(\"EVENTS_VOLUME\")\n",
        "SIMULATOR_SCHEMA = dbutils.widgets.get(\"SIMULATOR_SCHEMA\")\n",
        "PIPELINE_SCHEDULE_MINUTES = int(dbutils.widgets.get(\"PIPELINE_SCHEDULE_MINUTES\"))\n",
        "\n",
        "# 0 = continuous mode, N > 0 = triggered mode with schedule every N minutes\n",
        "continuous_mode = (PIPELINE_SCHEDULE_MINUTES == 0)\n",
        "\n",
        "print(f\"Pipeline mode: {'Continuous' if continuous_mode else f'Triggered (every {PIPELINE_SCHEDULE_MINUTES} minutes)'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "69dfc806-58b2-43d5-b13e-29d2661ca099",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "from databricks.sdk import WorkspaceClient\n",
        "from databricks.sdk.service import pipelines\n",
        "\n",
        "w = WorkspaceClient()\n",
        "\n",
        "PIPELINE_NAME = \"Order Items Medallion Declarative Pipeline\"\n",
        "\n",
        "root_abs_path = os.path.abspath(\"../pipelines/order_items\")\n",
        "root_dbx_path = root_abs_path.replace(\n",
        "    os.environ.get(\"DATABRICKS_WORKSPACE_ROOT\", \"/Workspace\"),\n",
        "    \"/Workspace\"\n",
        ")\n",
        "\n",
        "\n",
        "def find_existing_pipeline_id(name: str, root_path: str) -> Optional[str]:\n",
        "    for pipeline_state in w.pipelines.list_pipelines():\n",
        "        pipeline_name = getattr(pipeline_state, \"name\", None)\n",
        "        pipeline_id = getattr(pipeline_state, \"pipeline_id\", None)\n",
        "\n",
        "        if pipeline_name != name or not pipeline_id:\n",
        "            continue\n",
        "\n",
        "        details = w.pipelines.get(pipeline_id=pipeline_id)\n",
        "        spec = details.spec\n",
        "        if spec and spec.root_path == root_path:\n",
        "            return pipeline_id\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "pipeline_id: Optional[str] = None\n",
        "all_events_table = f\"{CATALOG}.lakeflow.all_events\"\n",
        "\n",
        "if spark.catalog.tableExists(all_events_table):\n",
        "    print(\n",
        "        f\"{all_events_table} already exists; skipping pipeline create/update to avoid table ownership conflicts.\"\n",
        "    )\n",
        "else:\n",
        "    existing_pipeline_id = find_existing_pipeline_id(PIPELINE_NAME, root_dbx_path)\n",
        "\n",
        "    pipeline_spec = dict(\n",
        "        catalog=CATALOG,\n",
        "        schema=\"lakeflow\",\n",
        "        continuous=continuous_mode,\n",
        "        name=PIPELINE_NAME,\n",
        "        serverless=True,\n",
        "        configuration={\n",
        "            \"RAW_DATA_CATALOG\": CATALOG,\n",
        "            \"RAW_DATA_SCHEMA\": SIMULATOR_SCHEMA,\n",
        "            \"RAW_DATA_VOLUME\": EVENTS_VOLUME,\n",
        "        },\n",
        "        root_path=root_dbx_path,\n",
        "        libraries=[\n",
        "            pipelines.PipelineLibrary(\n",
        "                glob=pipelines.PathPattern(include=f\"{root_dbx_path}/**\")\n",
        "            )\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    if existing_pipeline_id:\n",
        "        w.pipelines.update(pipeline_id=existing_pipeline_id, **pipeline_spec)\n",
        "        pipeline_id = existing_pipeline_id\n",
        "        print(f\"Updated existing pipeline_id={pipeline_id} (continuous={continuous_mode})\")\n",
        "    else:\n",
        "        created = w.pipelines.create(allow_duplicate_names=False, **pipeline_spec)\n",
        "        pipeline_id = created.pipeline_id\n",
        "        print(f\"Created pipeline_id={pipeline_id} (continuous={continuous_mode})\")\n",
        "\n",
        "    # If triggered mode, create a scheduled job to run pipeline updates\n",
        "    if not continuous_mode:\n",
        "        import databricks.sdk.service.jobs as j\n",
        "\n",
        "        cron_expression = f\"0 0/{PIPELINE_SCHEDULE_MINUTES} * * * ?\"\n",
        "\n",
        "        pipeline_job = w.jobs.create(\n",
        "            name=f\"Pipeline Update Scheduler (every {PIPELINE_SCHEDULE_MINUTES} min)\",\n",
        "            tasks=[\n",
        "                j.Task(\n",
        "                    task_key=\"update_pipeline\",\n",
        "                    pipeline_task=j.PipelineTask(\n",
        "                        pipeline_id=pipeline_id\n",
        "                    )\n",
        "                )\n",
        "            ],\n",
        "            schedule=j.CronSchedule(\n",
        "                quartz_cron_expression=cron_expression,\n",
        "                timezone_id=\"UTC\",\n",
        "                pause_status=j.PauseStatus.UNPAUSED\n",
        "            )\n",
        "        )\n",
        "\n",
        "        print(f\"Created scheduled job_id={pipeline_job.job_id} to run pipeline every {PIPELINE_SCHEDULE_MINUTES} minutes\")\n",
        "\n",
        "        # Register the job with uc_state\n",
        "        import sys\n",
        "        sys.path.append('../utils')\n",
        "        from uc_state import add\n",
        "        add(CATALOG, \"jobs\", pipeline_job)\n",
        "\n",
        "        # Run immediately once\n",
        "        w.jobs.run_now(job_id=pipeline_job.job_id)\n",
        "        print(\"Started initial pipeline run\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "78fe42c2-32da-4526-ac02-99094ac85d0a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# wait for the tables to be created\n",
        "# future stages may require their existence before being able to be run\n",
        "\n",
        "import time\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        if spark.catalog.tableExists(f\"{CATALOG}.lakeflow.all_events\"):\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "    time.sleep(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "24a507e5-644b-42de-91d4-3214146ae409",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# Also add to UC-state\n",
        "if pipeline_id:\n",
        "    import sys\n",
        "    sys.path.append('../utils')\n",
        "    from uc_state import add\n",
        "\n",
        "    add(CATALOG, \"pipelines\", {\"pipeline_id\": pipeline_id, \"name\": PIPELINE_NAME})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "3"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "lakeflow",
      "widgets": {
        "CATALOG": {
          "currentValue": "",
          "nuid": "1b879c13-e967-45d3-9848-122420dc1369",
          "typedWidgetInfo": {
            "autoCreated": false,
            "defaultValue": "",
            "label": null,
            "name": "CATALOG",
            "options": {
              "validationRegex": null,
              "widgetDisplayType": "Text"
            },
            "parameterDataType": "String"
          },
          "widgetInfo": {
            "defaultValue": "",
            "label": null,
            "name": "CATALOG",
            "options": {
              "autoCreated": false,
              "validationRegex": null,
              "widgetType": "text"
            },
            "widgetType": "text"
          }
        },
        "EVENTS_VOLUME": {
          "currentValue": "",
          "nuid": "267538f4-e4bf-4e45-a232-83760867245e",
          "typedWidgetInfo": {
            "autoCreated": false,
            "defaultValue": "",
            "label": null,
            "name": "EVENTS_VOLUME",
            "options": {
              "validationRegex": null,
              "widgetDisplayType": "Text"
            },
            "parameterDataType": "String"
          },
          "widgetInfo": {
            "defaultValue": "",
            "label": null,
            "name": "EVENTS_VOLUME",
            "options": {
              "autoCreated": false,
              "validationRegex": null,
              "widgetType": "text"
            },
            "widgetType": "text"
          }
        },
        "SIMULATOR_SCHEMA": {
          "currentValue": "",
          "nuid": "1eebe964-7af5-44a0-9d82-804fa88755f7",
          "typedWidgetInfo": {
            "autoCreated": false,
            "defaultValue": "",
            "label": null,
            "name": "SIMULATOR_SCHEMA",
            "options": {
              "validationRegex": null,
              "widgetDisplayType": "Text"
            },
            "parameterDataType": "String"
          },
          "widgetInfo": {
            "defaultValue": "",
            "label": null,
            "name": "SIMULATOR_SCHEMA",
            "options": {
              "autoCreated": false,
              "validationRegex": null,
              "widgetType": "text"
            },
            "widgetType": "text"
          }
        }
      }
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}